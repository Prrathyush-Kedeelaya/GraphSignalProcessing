{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cd56faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.5006 - accuracy: 0.4762 - val_loss: 0.7812 - val_accuracy: 0.7724\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8739 - accuracy: 0.7085 - val_loss: 0.5031 - val_accuracy: 0.8567\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6817 - accuracy: 0.7817 - val_loss: 0.3866 - val_accuracy: 0.8865\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5761 - accuracy: 0.8181 - val_loss: 0.3288 - val_accuracy: 0.9028\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5149 - accuracy: 0.8398 - val_loss: 0.2920 - val_accuracy: 0.9118\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4690 - accuracy: 0.8536 - val_loss: 0.2699 - val_accuracy: 0.9191\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4377 - accuracy: 0.8639 - val_loss: 0.2571 - val_accuracy: 0.9244\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4103 - accuracy: 0.8737 - val_loss: 0.2428 - val_accuracy: 0.9254\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3897 - accuracy: 0.8793 - val_loss: 0.2368 - val_accuracy: 0.9263\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3765 - accuracy: 0.8843 - val_loss: 0.2253 - val_accuracy: 0.9317\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3567 - accuracy: 0.8893 - val_loss: 0.2192 - val_accuracy: 0.9333\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3414 - accuracy: 0.8958 - val_loss: 0.2171 - val_accuracy: 0.9366\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3330 - accuracy: 0.8976 - val_loss: 0.2126 - val_accuracy: 0.9354\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3244 - accuracy: 0.8983 - val_loss: 0.2069 - val_accuracy: 0.9374\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3125 - accuracy: 0.9032 - val_loss: 0.2066 - val_accuracy: 0.9387\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3096 - accuracy: 0.9041 - val_loss: 0.2002 - val_accuracy: 0.9396\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2999 - accuracy: 0.9062 - val_loss: 0.2009 - val_accuracy: 0.9412\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2972 - accuracy: 0.9071 - val_loss: 0.1950 - val_accuracy: 0.9424\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2866 - accuracy: 0.9127 - val_loss: 0.1949 - val_accuracy: 0.9414\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2833 - accuracy: 0.9124 - val_loss: 0.1914 - val_accuracy: 0.9421\n",
      "313/313 [==============================] - 0s 676us/step - loss: 0.1914 - accuracy: 0.9421\n",
      "Test accuracy with graph filtering: [0.19138123095035553, 0.9420999884605408]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import os\n",
    "import numpy as np\n",
    "import pygsp\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Get the working directory path\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Load the MNIST dataset and normalize the pixel values\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data(path=current_dir + '/mnist.zip')\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define a simple graph filtering function\n",
    "def graph_filtering(signal, laplacian):\n",
    "    dense_laplacian = laplacian.toarray()  # Convert to dense matrix\n",
    "    filtered_signal = np.dot(dense_laplacian, signal).flatten()  # Perform graph filtering\n",
    "    return filtered_signal\n",
    "\n",
    "# Create a 1D chain graph manually\n",
    "N = x_train.shape[1] * x_train.shape[2] \n",
    "adjacency = np.eye(N, k=1) + np.eye(N, k=-1)\n",
    "G = pygsp.graphs.Graph(adjacency)\n",
    "\n",
    "# Compute the Laplacian matrix of the graph\n",
    "G.compute_laplacian(\"normalized\")\n",
    "\n",
    "# Perform graph filtering on the training data\n",
    "x_train_flattened = x_train.reshape(-1, N) # Flattened image size\n",
    "x_train_filtered = np.zeros_like(x_train_flattened)\n",
    "for i in range(len(x_train)):\n",
    "    x_train_filtered[i, :] = graph_filtering(x_train_flattened[i, :], G.L)\n",
    "\n",
    "# Perform graph filtering on the test data\n",
    "x_test_flattened = x_test.reshape(-1, N)\n",
    "x_test_filtered = np.zeros_like(x_test_flattened)\n",
    "for i in range(len(x_test)):\n",
    "    x_test_filtered[i, :] = graph_filtering(x_test_flattened[i, :], G.L)\n",
    "\n",
    "# Create the model with the flattened input shape\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(N,)))  # Flattened input shape\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile and train the model with the filtered data\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train_filtered, y_train, epochs=20, batch_size=128, validation_data=(x_test_filtered, y_test))\n",
    "\n",
    "# Evaluate the model on the filtered test data\n",
    "test_accuracy = model.evaluate(x_test_filtered, y_test)\n",
    "print(f'Test accuracy with graph filtering: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b456942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x_train_flattened: (60000, 784)\n",
      "Matrix representation of flattened images:\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215687\n",
      " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
      " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313726\n",
      " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13725491 0.94509804\n",
      " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      " 0.5882353  0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.5803922\n",
      " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058825\n",
      " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
      " 0.3137255  0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333336 0.99215686\n",
      " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "x_train_flattened_size = x_train_flattened.shape\n",
    "print(f\"Size of x_train_flattened: {x_train_flattened_size}\")\n",
    "\n",
    "print(\"Matrix representation of flattened images:\")\n",
    "for i in range(1):  # Print the first 10 images for demonstration\n",
    "    print(x_train_flattened[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7616ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Laplacian matrix: (784, 784)\n",
      "Laplacian matrix:\n",
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t-0.7071067811865476\n",
      "  (1, 0)\t-0.7071067811865476\n",
      "  (1, 1)\t1.0\n",
      "  (1, 2)\t-0.5000000000000001\n",
      "  (2, 1)\t-0.5000000000000001\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t-0.5000000000000001\n",
      "  (3, 2)\t-0.5000000000000001\n",
      "  (3, 3)\t1.0\n",
      "  (3, 4)\t-0.5000000000000001\n",
      "  (4, 3)\t-0.5000000000000001\n",
      "  (4, 4)\t1.0\n",
      "  (4, 5)\t-0.5000000000000001\n",
      "  (5, 4)\t-0.5000000000000001\n",
      "  (5, 5)\t1.0\n",
      "  (5, 6)\t-0.5000000000000001\n",
      "  (6, 5)\t-0.5000000000000001\n",
      "  (6, 6)\t1.0\n",
      "  (6, 7)\t-0.5000000000000001\n",
      "  (7, 6)\t-0.5000000000000001\n",
      "  (7, 7)\t1.0\n",
      "  (7, 8)\t-0.5000000000000001\n",
      "  (8, 7)\t-0.5000000000000001\n",
      "  (8, 8)\t1.0\n",
      "  :\t:\n",
      "  (775, 775)\t1.0\n",
      "  (775, 776)\t-0.5000000000000001\n",
      "  (776, 775)\t-0.5000000000000001\n",
      "  (776, 776)\t1.0\n",
      "  (776, 777)\t-0.5000000000000001\n",
      "  (777, 776)\t-0.5000000000000001\n",
      "  (777, 777)\t1.0\n",
      "  (777, 778)\t-0.5000000000000001\n",
      "  (778, 777)\t-0.5000000000000001\n",
      "  (778, 778)\t1.0\n",
      "  (778, 779)\t-0.5000000000000001\n",
      "  (779, 778)\t-0.5000000000000001\n",
      "  (779, 779)\t1.0\n",
      "  (779, 780)\t-0.5000000000000001\n",
      "  (780, 779)\t-0.5000000000000001\n",
      "  (780, 780)\t1.0\n",
      "  (780, 781)\t-0.5000000000000001\n",
      "  (781, 780)\t-0.5000000000000001\n",
      "  (781, 781)\t1.0\n",
      "  (781, 782)\t-0.5000000000000001\n",
      "  (782, 781)\t-0.5000000000000001\n",
      "  (782, 782)\t1.0\n",
      "  (782, 783)\t-0.7071067811865476\n",
      "  (783, 782)\t-0.7071067811865476\n",
      "  (783, 783)\t1.0\n"
     ]
    }
   ],
   "source": [
    "laplacian_size = G.L.shape\n",
    "print(f\"Size of Laplacian matrix: {laplacian_size}\")\n",
    "print(\"Laplacian matrix:\")\n",
    "print(G.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1526dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
