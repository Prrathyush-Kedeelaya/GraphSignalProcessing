{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cd56faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.5129 - accuracy: 0.4731 - val_loss: 0.7721 - val_accuracy: 0.7740\n",
      "Epoch 2/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.8791 - accuracy: 0.7118 - val_loss: 0.4960 - val_accuracy: 0.8541\n",
      "Epoch 3/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.6872 - accuracy: 0.7797 - val_loss: 0.3935 - val_accuracy: 0.8863\n",
      "Epoch 4/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5920 - accuracy: 0.8146 - val_loss: 0.3444 - val_accuracy: 0.8966\n",
      "Epoch 5/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5261 - accuracy: 0.8358 - val_loss: 0.3081 - val_accuracy: 0.9080\n",
      "Epoch 6/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4885 - accuracy: 0.8477 - val_loss: 0.2844 - val_accuracy: 0.9140\n",
      "Epoch 7/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4563 - accuracy: 0.8590 - val_loss: 0.2673 - val_accuracy: 0.9189\n",
      "Epoch 8/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4203 - accuracy: 0.8681 - val_loss: 0.2549 - val_accuracy: 0.9229\n",
      "Epoch 9/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3999 - accuracy: 0.8772 - val_loss: 0.2366 - val_accuracy: 0.9306\n",
      "Epoch 10/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3821 - accuracy: 0.8803 - val_loss: 0.2298 - val_accuracy: 0.9312\n",
      "Epoch 11/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3660 - accuracy: 0.8871 - val_loss: 0.2270 - val_accuracy: 0.9316\n",
      "Epoch 12/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3542 - accuracy: 0.8910 - val_loss: 0.2221 - val_accuracy: 0.9338\n",
      "Epoch 13/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3400 - accuracy: 0.8949 - val_loss: 0.2139 - val_accuracy: 0.9353\n",
      "Epoch 14/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3324 - accuracy: 0.8947 - val_loss: 0.2107 - val_accuracy: 0.9356\n",
      "Epoch 15/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3204 - accuracy: 0.9006 - val_loss: 0.2064 - val_accuracy: 0.9385\n",
      "Epoch 16/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3137 - accuracy: 0.9030 - val_loss: 0.2020 - val_accuracy: 0.9390\n",
      "Epoch 17/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3065 - accuracy: 0.9048 - val_loss: 0.2027 - val_accuracy: 0.9376\n",
      "Epoch 18/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2947 - accuracy: 0.9093 - val_loss: 0.2000 - val_accuracy: 0.9419\n",
      "Epoch 19/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2900 - accuracy: 0.9091 - val_loss: 0.1942 - val_accuracy: 0.9415\n",
      "Epoch 20/20\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2865 - accuracy: 0.9111 - val_loss: 0.1958 - val_accuracy: 0.9423\n",
      "313/313 [==============================] - 0s 641us/step - loss: 0.1958 - accuracy: 0.9423\n",
      "Test accuracy with graph filtering: [0.19580550491809845, 0.942300021648407]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import os\n",
    "import numpy as np\n",
    "import pygsp\n",
    "\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Get the working directory path\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Load the MNIST dataset and normalize the pixel values\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data(path=current_dir + '/mnist.zip')\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0\n",
    "\n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "# Define a simple graph filtering function\n",
    "def graph_filtering(signal, laplacian):\n",
    "    dense_laplacian = laplacian.toarray()  # Convert to dense matrix\n",
    "    filtered_signal = np.dot(dense_laplacian, signal).flatten()  # Perform graph filtering\n",
    "    return filtered_signal\n",
    "\n",
    "# Create a 1D chain graph manually\n",
    "N = x_train_flattened.shape[1]  # Flattened image size\n",
    "adjacency = np.eye(N, k=1) + np.eye(N, k=-1)\n",
    "G = pygsp.graphs.Graph(adjacency)\n",
    "\n",
    "# Compute the Laplacian matrix of the graph\n",
    "G.compute_laplacian(\"normalized\")\n",
    "\n",
    "# Perform graph filtering on the training data\n",
    "x_train_flattened = x_train.reshape(-1, N)\n",
    "x_train_filtered = np.zeros_like(x_train_flattened)\n",
    "for i in range(len(x_train)):\n",
    "    x_train_filtered[i, :] = graph_filtering(x_train_flattened[i, :], G.L)\n",
    "\n",
    "# Perform graph filtering on the test data\n",
    "x_test_flattened = x_test.reshape(-1, N)\n",
    "x_test_filtered = np.zeros_like(x_test_flattened)\n",
    "for i in range(len(x_test)):\n",
    "    x_test_filtered[i, :] = graph_filtering(x_test_flattened[i, :], G.L)\n",
    "\n",
    "# Create the model with the flattened input shape\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(N,)))  # Flattened input shape\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile and train the model with the filtered data\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train_filtered, y_train, epochs=20, batch_size=128, validation_data=(x_test_filtered, y_test))\n",
    "\n",
    "# Evaluate the model on the filtered test data\n",
    "test_accuracy = model.evaluate(x_test_filtered, y_test)\n",
    "print(f'Test accuracy with graph filtering: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b456942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix representation of flattened images:\n",
      "[0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.01176471 0.07058824 0.07058824 0.07058824\n",
      " 0.49411765 0.53333336 0.6862745  0.10196079 0.6509804  1.\n",
      " 0.96862745 0.49803922 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.11764706 0.14117648 0.36862746 0.6039216\n",
      " 0.6666667  0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.88235295 0.6745098  0.99215686 0.9490196  0.7647059  0.2509804\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.19215687\n",
      " 0.93333334 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.9843137  0.3647059  0.32156864\n",
      " 0.32156864 0.21960784 0.15294118 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.07058824 0.85882354 0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.99215686 0.7764706  0.7137255\n",
      " 0.96862745 0.94509804 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.3137255  0.6117647  0.41960785 0.99215686\n",
      " 0.99215686 0.8039216  0.04313726 0.         0.16862746 0.6039216\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.05490196 0.00392157 0.6039216  0.99215686 0.3529412\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.54509807 0.99215686 0.74509805 0.00784314 0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.04313726\n",
      " 0.74509805 0.99215686 0.27450982 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.13725491 0.94509804\n",
      " 0.88235295 0.627451   0.42352942 0.00392157 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.31764707 0.9411765  0.99215686\n",
      " 0.99215686 0.46666667 0.09803922 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.1764706  0.7294118  0.99215686 0.99215686\n",
      " 0.5882353  0.10588235 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.0627451  0.3647059  0.9882353  0.99215686 0.73333335\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.9764706  0.99215686 0.9764706  0.2509804  0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.18039216 0.50980395 0.7176471  0.99215686\n",
      " 0.99215686 0.8117647  0.00784314 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.15294118 0.5803922\n",
      " 0.8980392  0.99215686 0.99215686 0.99215686 0.98039216 0.7137255\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.09411765 0.44705883 0.8666667  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.7882353  0.30588236 0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.09019608 0.25882354 0.8352941  0.99215686\n",
      " 0.99215686 0.99215686 0.99215686 0.7764706  0.31764707 0.00784314\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.07058824 0.67058825\n",
      " 0.85882354 0.99215686 0.99215686 0.99215686 0.99215686 0.7647059\n",
      " 0.3137255  0.03529412 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.21568628 0.6745098  0.8862745  0.99215686 0.99215686 0.99215686\n",
      " 0.99215686 0.95686275 0.52156866 0.04313726 0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.53333336 0.99215686\n",
      " 0.99215686 0.99215686 0.83137256 0.5294118  0.5176471  0.0627451\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.        ]\n",
      "Size of x_train_flattened: (60000, 784)\n",
      "Size of Laplacian matrix: (784, 784)\n"
     ]
    }
   ],
   "source": [
    "print(\"Matrix representation of flattened images:\")\n",
    "for i in range(1):  # Print the first 10 images for demonstration\n",
    "    print(x_train_flattened[i])\n",
    "    \n",
    "x_train_flattened_size = x_train_flattened.shape\n",
    "print(f\"Size of x_train_flattened: {x_train_flattened_size}\")\n",
    "laplacian_size = G.L.shape\n",
    "print(f\"Size of Laplacian matrix: {laplacian_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7616ee7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Laplacian matrix:\n",
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t-0.7071067811865476\n",
      "  (1, 0)\t-0.7071067811865476\n",
      "  (1, 1)\t1.0\n",
      "  (1, 2)\t-0.5000000000000001\n",
      "  (2, 1)\t-0.5000000000000001\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t-0.5000000000000001\n",
      "  (3, 2)\t-0.5000000000000001\n",
      "  (3, 3)\t1.0\n",
      "  (3, 4)\t-0.5000000000000001\n",
      "  (4, 3)\t-0.5000000000000001\n",
      "  (4, 4)\t1.0\n",
      "  (4, 5)\t-0.5000000000000001\n",
      "  (5, 4)\t-0.5000000000000001\n",
      "  (5, 5)\t1.0\n",
      "  (5, 6)\t-0.5000000000000001\n",
      "  (6, 5)\t-0.5000000000000001\n",
      "  (6, 6)\t1.0\n",
      "  (6, 7)\t-0.5000000000000001\n",
      "  (7, 6)\t-0.5000000000000001\n",
      "  (7, 7)\t1.0\n",
      "  (7, 8)\t-0.5000000000000001\n",
      "  (8, 7)\t-0.5000000000000001\n",
      "  (8, 8)\t1.0\n",
      "  :\t:\n",
      "  (775, 775)\t1.0\n",
      "  (775, 776)\t-0.5000000000000001\n",
      "  (776, 775)\t-0.5000000000000001\n",
      "  (776, 776)\t1.0\n",
      "  (776, 777)\t-0.5000000000000001\n",
      "  (777, 776)\t-0.5000000000000001\n",
      "  (777, 777)\t1.0\n",
      "  (777, 778)\t-0.5000000000000001\n",
      "  (778, 777)\t-0.5000000000000001\n",
      "  (778, 778)\t1.0\n",
      "  (778, 779)\t-0.5000000000000001\n",
      "  (779, 778)\t-0.5000000000000001\n",
      "  (779, 779)\t1.0\n",
      "  (779, 780)\t-0.5000000000000001\n",
      "  (780, 779)\t-0.5000000000000001\n",
      "  (780, 780)\t1.0\n",
      "  (780, 781)\t-0.5000000000000001\n",
      "  (781, 780)\t-0.5000000000000001\n",
      "  (781, 781)\t1.0\n",
      "  (781, 782)\t-0.5000000000000001\n",
      "  (782, 781)\t-0.5000000000000001\n",
      "  (782, 782)\t1.0\n",
      "  (782, 783)\t-0.7071067811865476\n",
      "  (783, 782)\t-0.7071067811865476\n",
      "  (783, 783)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Laplacian matrix:\")\n",
    "print(G.L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1526dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
