{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8406dcd0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 784000 is different from 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Perform optimization with iterative updates\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iterations):\n\u001b[1;32m---> 84\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mminimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mM2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL-BFGS-B\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[43m                      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaxiter\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgtol\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m# Extract the final optimized signal matrix S and reshape it to its original shape\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     optimized_S \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mx\u001b[38;5;241m.\u001b[39mreshape(S\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    693\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml-bfgs-b\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 696\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    697\u001b[0m                            callback\u001b[38;5;241m=\u001b[39mcallback, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m meth \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtnc\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    699\u001b[0m     res \u001b[38;5;241m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[38;5;241m=\u001b[39mcallback,\n\u001b[0;32m    700\u001b[0m                         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:305\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m         iprint \u001b[38;5;241m=\u001b[39m disp\n\u001b[1;32m--> 305\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_bounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    309\u001b[0m func_and_grad \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun_and_grad\n\u001b[0;32m    311\u001b[0m fortran_int \u001b[38;5;241m=\u001b[39m _lbfgsb\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mintvar\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_optimize.py:332\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[0;32m    328\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[1;32m--> 332\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[1;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(grad):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Cell \u001b[1;32mIn[4], line 69\u001b[0m, in \u001b[0;36mcost_function\u001b[1;34m(S, alpha, M1, M2)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcost_function\u001b[39m(S, alpha, M1, M2):\n\u001b[1;32m---> 69\u001b[0m     CS_known \u001b[38;5;241m=\u001b[39m \u001b[43mdiagonal_matrix_C\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mS\u001b[49m\n\u001b[0;32m     70\u001b[0m     CS_algorithm \u001b[38;5;241m=\u001b[39m diagonal_matrix_C \u001b[38;5;241m@\u001b[39m S\n\u001b[0;32m     71\u001b[0m     diff_CS \u001b[38;5;241m=\u001b[39m CS_known \u001b[38;5;241m-\u001b[39m CS_algorithm\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 784000 is different from 784)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.optimize import minimize\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Load and preprocess MNIST data (using only the first 1000 samples)\n",
    "(x_train, y_train), (_, _) = mnist.load_data()\n",
    "num_samples = 1000  # Number of samples to use\n",
    "x_train = x_train[:num_samples].reshape(num_samples, -1) / 255.0\n",
    "y_train = y_train[:num_samples]\n",
    "\n",
    "# Load the test data from the CSV file\n",
    "file_path = 'C:/Users/pc/Documents/train.csv'\n",
    "test_data = pd.read_csv(file_path, delimiter=',', header=0)\n",
    "\n",
    "# Ensure the test data is preprocessed in the same way as the training data\n",
    "test_data = test_data.values / 255.0  # Normalize the data if needed\n",
    "\n",
    "# Define the number of nearest neighbors (k) and a distance threshold (adjust as needed)\n",
    "k = 5\n",
    "distance_threshold = 0.2  # Adjust the threshold as needed\n",
    "\n",
    "# Compute pairwise Euclidean distances\n",
    "distances = squareform(pdist(x_train, metric='euclidean'))\n",
    "\n",
    "# Create the k-NN graph based on distance threshold\n",
    "adjacency = (distances < distance_threshold).astype(int)\n",
    "\n",
    "# Create a NetworkX graph from the adjacency matrix\n",
    "G = nx.Graph(adjacency)\n",
    "\n",
    "# Normalize the graph (Laplacian normalization)\n",
    "degree_matrix = np.diag(np.sum(adjacency, axis=1))  # Degree matrix\n",
    "normalized_adjacency = np.linalg.inv(np.sqrt(degree_matrix)) @ adjacency @ np.linalg.inv(np.sqrt(degree_matrix))\n",
    "\n",
    "# Define optimization hyperparameters\n",
    "alpha = 0.1\n",
    "epsilon = 1e-6\n",
    "max_iterations = 10\n",
    "\n",
    "# Define your graph signal matrix `S` based on class labels (or other features)\n",
    "num_features = 28 * 28\n",
    "S = np.random.randn(num_samples, num_features)\n",
    "\n",
    "# Define C matrix based on class assignment (assuming you've created C based on class labels)\n",
    "class_assignment = y_train\n",
    "C = np.diag(class_assignment)\n",
    "\n",
    "# Ensure that S has the correct shape (num_samples, num_features)\n",
    "num_samples, num_features = S.shape\n",
    "\n",
    "# Define diagonal_matrix_C based on the number of samples\n",
    "diagonal_matrix_C = np.diag(np.diag(S))\n",
    "\n",
    "\n",
    "# Precompute matrices M1 and M2 based on the normalized adjacency matrix\n",
    "M1 = (np.eye(num_samples) - normalized_adjacency).T @ (np.eye(num_samples) - normalized_adjacency)\n",
    "M2 = diagonal_matrix_C.T @ diagonal_matrix_C\n",
    "\n",
    "# Define your optimization objective and gradient functions\n",
    "def cost_function(S, alpha, M1, M2):\n",
    "    CS_known = diagonal_matrix_C @ S\n",
    "    CS_algorithm = diagonal_matrix_C @ S\n",
    "    diff_CS = CS_known - CS_algorithm\n",
    "    J = 0.5 * (np.linalg.norm((S - (M1 + 2 * alpha * M2) @ S) @ normalized_adjacency)**2 +\n",
    "               alpha * np.linalg.norm(diff_CS)**2)\n",
    "    return J\n",
    "\n",
    "def gradient(S, alpha, M1, M2):\n",
    "    CS_known = diagonal_matrix_C @ S\n",
    "    CS_algorithm = diagonal_matrix_C @ S\n",
    "    gradient_J = (M1 + 2 * alpha * M2) @ S - CS_known + CS_algorithm\n",
    "    return gradient_J\n",
    "\n",
    "# Perform optimization with iterative updates\n",
    "for iteration in range(max_iterations):\n",
    "    result = minimize(cost_function, S.flatten(), args=(alpha, M1, M2), method='L-BFGS-B', jac=gradient,\n",
    "                      options={'maxiter': max_iterations, 'gtol': epsilon})\n",
    "\n",
    "    # Extract the final optimized signal matrix S and reshape it to its original shape\n",
    "    optimized_S = result.x.reshape(S.shape)\n",
    "\n",
    "# Neural Network\n",
    "# Reshape the data and normalize it\n",
    "X_train = x_train\n",
    "X_test = test_data\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = 10\n",
    "Y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(num_features,)))  # Input shape matches the number of features\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes))  # Output layer with num_classes neurons\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=10, verbose=1)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "class_probabilities = model.predict(X_test)\n",
    "\n",
    "# Determine the predicted classes based on the highest probability\n",
    "predicted_classes = np.argmax(class_probabilities, axis=1)\n",
    "\n",
    "\n",
    "# Determine the predicted classes based on the highest probability\n",
    "predicted_classes = np.argmax(class_probabilities, axis=1)\n",
    "\n",
    "# Now you can continue with your evaluation and visualization code\n",
    "correct_indices = np.nonzero(predicted_classes == y_test)[0]\n",
    "incorrect_indices = np.nonzero(predicted_classes != y_test)[0]\n",
    "\n",
    "# Visualize correct predictions\n",
    "plt.figure()\n",
    "for i, correct in enumerate(correct_indices[:9]):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(X_test[correct].reshape(28, 28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[correct], y_test[correct]))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Visualize incorrect predictions\n",
    "plt.figure()\n",
    "for i, incorrect in enumerate(incorrect_indices[:9]):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(X_test[incorrect].reshape(28, 28), cmap='gray', interpolation='none')\n",
    "    plt.title(\"Predicted {}, Class {}\".format(predicted_classes[incorrect], y_test[incorrect]))\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Make predictions on the test data\n",
    "results = model.predict(test_data)\n",
    "results = np.argmax(results, axis=1)\n",
    "results = pd.Series(results, name=\"Label\")\n",
    "submission = pd.concat([pd.Series(range(1, len(results) + 1), name=\"ImageId\"), results], axis=1)\n",
    "\n",
    "# Save the submission to a CSV file\n",
    "submission.to_csv(\"submission3.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a48afa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
